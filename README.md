# audiojack
CIS 6200 - Advanced Deep Learning final project

---
# SyllaBERT: Syllable-Aware HuBERT Pretraining

SyllaBERT is a syllable-aware variant of HuBERT that operates on raw audio and predicts discrete pseudo-labels over syllables rather than fixed frames. This enables more linguistically grounded modeling of spoken language with better data and parameter efficiency.

---

## ğŸ§± Pipeline Overview

1. **Syllable Segmentation:** Detect syllables in raw speech using a valley-peak-valley heuristic over a modulation-based amplitude envelope  
2. **MFCC Extraction:** Extract a single MFCC vector per syllable  
3. **Clustering:** Apply k-means to the MFCCs to generate pseudo-labels  
4. **Training:** Train a HuBERT-style model using raw audio input and masked syllable-label prediction

---

## âš™ï¸ Environment Setup

```bash
conda create -n syllabert python=3.11
conda activate syllabert

pip install torch torchaudio librosa tqdm scipy scikit-learn textgrid
```

---

## ğŸ§ 1. Download LibriSpeech 100h Subset

```bash
mkdir -p ~/datasets/LibriSpeech
cd ~/datasets/LibriSpeech
wget https://www.openslr.org/resources/12/train-clean-100.tar.gz
tar -xvzf train-clean-100.tar.gz
```

---

## ğŸ™ 2. Segment Audio and Extract MFCCs

```bash
python segment_mean_mfcc_librispeech.py \
  --librispeech_root ~/datasets/LibriSpeech/train-clean-100 \
  --output_dir ./data/syllabert_clean100/features \
  --manifest_file ./data/syllabert_clean100/features/manifest.jsonl
```

This script:
- Segments each utterance into syllables using an amplitude-based method
- Computes a single MFCC vector per syllable
- Saves `.npy` feature files and a `manifest.jsonl` describing them

---

## ğŸ”€ 3. Cluster Syllable MFCCs

```bash
python cluster_syllable_vectors.py \
  --manifest ./data/syllabert_clean100/features/manifest.jsonl \
  --output_dir ./data/syllabert_clean100/clustering \
  --n_clusters 100
```

This script:
- Loads all MFCC vectors from the manifest
- Applies k-means clustering with `n_clusters`
- Writes a new manifest with cluster IDs as pseudo-labels

Output:
```
./data/syllabert_clean100/clustering/labeled_manifest.jsonl
```

---

## ğŸš€ 4. Train the SyllaBERT Model

```bash
python train_syllabert.py
```

This script:
- Loads raw waveforms and syllable boundary metadata
- Applies a convolutional frontend + transformer encoder
- Uses HuBERT-style masking (65% of syllables per batch)
- Computes loss only over masked syllables
- Logs batch accuracy and loss
- Saves model checkpoints after each epoch:
  ```
  checkpoints/syllabert_wave_epochN.pt
  checkpoints/syllabert_wave_latest.pt
  ```

---

## ğŸ“‚ Directory Layout

```
data/
â””â”€â”€ syllabert_clean100/
    â”œâ”€â”€ features/
    â”‚   â”œâ”€â”€ <utterance_id>.npy          # MFCC vectors
    â”‚   â””â”€â”€ manifest.jsonl              # Metadata (audio paths, timings, features)
    â””â”€â”€ clustering/
        â”œâ”€â”€ kmeans_100.pkl              # KMeans model
        â””â”€â”€ labeled_manifest.jsonl      # MFCCs with cluster labels
```

---

## ğŸ“’ Notes

- Syllables are pooled from raw waveforms; targets are cluster IDs over syllable MFCCs.
- The architecture matches HuBERT, but the learning unit is a syllable.
- Masking and loss computation follow HuBERTâ€™s 65% random span masking strategy â€” adapted for syllables.

---

## ğŸ“š References

- Hsu et al., 2021. [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447)
```